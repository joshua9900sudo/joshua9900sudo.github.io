---
title: "[EEE4610] Explore Sparse Attention Mechanisms"
date: 2025-03-03 10:07:00  # Created date
last_modified_at: 2025-03-04 10:10:00  # Modified date
---

Our team suggested some ideas, and our graduate assistant guided our research direction as follows:  
- Refer to some accelerator papers on existing sparsity operations and consider how to apply them to PIM.  
- Dynamically offload computational burdens from the GPU to PIM. This could be achieved by reviewing existing papers and proposing improvements.  

We must choose one of the following topics:  
1. Sparse Attention  
2. Dynamically Offloading from GPU to PIM  

First, let's take a look at  
# Sparse Attention Mechanism using PIM
- Key Characteristics
	1. Reducing compputational loads by zeroing out less important attention scores
	2. Leveraging PIM to accelerate sparse matrix operations



